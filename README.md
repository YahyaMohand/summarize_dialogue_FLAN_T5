# summarize_dialogue_FLAN_T5
We will do the dialogue summarization task using generative AI. You will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task you need.

1 - Zero Shot Inference: In this step, you'll input dialogues without any explicit instructions or guidance about the desired output. The model will generate summaries based solely on the input dialogues and its general language knowledge.

2 - One Shot Inference: Here, you'll provide a single instruction or prompt to guide the model's output. This prompt will likely indicate that the model should generate a summary for the given dialogue. The aim is to observe how a single, concise instruction can affect the model's performance.

3 - Few Shot Inference: In this stage, you'll provide a few examples of dialogues along with their corresponding summaries. The model will use these examples to learn how to generate summaries for similar dialogues. This approach leverages the few-shot learning capability of the model.

4 - Prompt Engineering: You'll experiment with crafting more sophisticated prompts to elicit desired outputs. This could involve refining the way you frame the instruction to guide the model's behavior more effectively. The goal is to enhance the quality and relevance of the generated summaries.

5 - Comparative Analysis: By comparing the outputs from the zero-shot, one-shot, and few-shot inferences, you'll gain insights into the effectiveness of prompt engineering. This analysis will help you understand how different levels of instruction impact the model's performance and how prompt engineering can improve it.

Overall, this experiment is a systematic way to approach prompt engineering and assess its impact on generative AI models' performance for the specific task of dialogue summarization. Keep in mind that the success of the experiment will depend on the choice of prompts, the model architecture used, and the quality of the training data. It's an exciting area of research and development, and the insights gained from this experiment can have broader implications for improving the usability and control of generative language models.





